| # | Название  | Ссылка |
| - | --------- | ------ |
| 1 | Bergstra, J., & Bengio, Y. (2012). Random search for hyper-parameter optimization. | [link](https://jmlr.org/papers/v13/bergstra12a.html) |
| 2 | Feurer, M., & Hutter, F. (2019). Hyperparameter optimization. | [link](https://link.springer.com/chapter/10.1007/978-3-030-05318-5_1) |
| 3 | Snoek, J., Larochelle, H., & Adams, R. P. (2012). Practical Bayesian optimization. | [link](https://papers.nips.cc/paper/2012/hash/4522e0563b15c3f1d641e722f65e5c56-Abstract.html) |
| 4 | Li, L., et al. (2016). Hyperband: A novel bandit-based approach. | [link](https://arxiv.org/abs/1603.06560) |
| 5 | Thornton, C., et al. (2013). Auto-WEKA: Combined selection and hyperparameter optimization. | [link](https://dl.acm.org/doi/10.1145/2487575.2487629) |
| 6 | Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2011). Sequential model-based optimization. | [link](https://link.springer.com/chapter/10.1007/978-3-642-25566-3_40) |
| 7 | Bergstra, J., Bardenet, R., Bengio, Y., & Kégl, B. (2011). Algorithms for Hyper-parameter optimization. | [link](https://papers.nips.cc/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html) |
| 8 | Eggensperger, K., Feurer, M., Hutter, F., Bergstra, J., Snoek, J., Hoos, H., & Leyton-Brown, K. (2013). Towards an empirical foundation for assessing Bayesian optimization of hyperparameters. | [link](https://www.tuebingen.mpg.de/fileadmin/user_upload/files/publication_pdf_file/file/43d0e3c7e6d7a5c9_PUBPDF_Eggensperger-2013-Towards-an-empirical-foundation-for-assessing-Bayesian-optimization-of-hyperparameters.pdf) |
| 9 | Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., & de Freitas, N. (2016). Taking the human out of the loop: A review of Bayesian optimization. | [link](https://www.pnas.org/content/112/29/8728) |
| 10 | Falkner, S., Klein, A., & Hutter, F. (2018). BOHB: Robust and Efficient Hyperparameter Optimization at Scale. | [link](https://arxiv.org/abs/1807.01774) |
| 11 | Probst, P., Bischl, B., & Boulesteix, A. L. (2019). Tunability: Importance of Hyperparameters of Machine Learning Algorithms. | [link](https://jmlr.org/papers/v20/18-444.html) |
| 12 | Klein, A., Falkner, S., Mansur, N., & Hutter, F. (2017). ROBO: A flexible and robust Bayesian optimization framework. | [link](https://arxiv.org/abs/1705.03565) |
| 13 | Olson, R. S., Moore, J. H. (2016). TPOT: A Tree-based Pipeline Optimization Tool for Automating Machine Learning. | [link](https://dl.acm.org/doi/10.1145/2908812.2908918) |
| 14 | Negrinho, R., Gordon, G. (2017). DeepArchitect: Automatically Designing and Training Deep Architectures. | [link](https://arxiv.org/abs/1704.08792) |
| 15 | Springenberg, J. T., Klein, A., Falkner, S., & Hutter, F. (2016). Bayesian optimization with robust Bayesian neural networks. | [link](https://arxiv.org/abs/1604.08323) |
| 16 | Young, D., Hazarika, D., Poria, S., Cambria, E. (2018). Recent Trends in Deep Learning Based Natural Language Processing. | [link](https://arxiv.org/abs/1708.02709) |  # Though not directly about hyperparam tuning, it discusses the importance of it in deep NLP models.
| 17 | Zoph, B., & Le, Q. V. (2017). Neural Architecture Search with Reinforcement Learning. | [link](https://arxiv.org/abs/1611.01578) |
| 18 | Smith, S. L., Kindermans, P. J., Ying, C., & Le, Q. V. (2018). Don't Decay the Learning Rate, Increase the Batch Size. | [link](https://arxiv.org/abs/1711.00489) |
